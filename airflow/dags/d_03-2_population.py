# d_03-2_population.py


import os
import sys

sys.path.append("/opt/airflow/tasks")
sys.path.append("/opt/airflow/utils")
sys.path.append("/opt/airflow/drivers")
from datetime import datetime, timedelta

from airflow.operators.python import PythonOperator

from airflow import DAG

# ==========================================================
# è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„ (airflow çš„ä¸Šä¸€å±¤)
# ==========================================================
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, ".."))  # airflow/ çš„ä¸Šä¸€å±¤

if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

# ==========================================================
# åŒ¯å…¥äººå£ ETL æ¨¡çµ„
# ==========================================================
from tasks.population.E_pop import fetch_raw_data
from tasks.population.L_pop import load
from tasks.population.T_pop import transform_population_data

# ==========================================================
# é è¨­åƒæ•¸
# ==========================================================
default_args = {
    "owner": "Ken",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=3),
}

# ==========================================================
# DAG è¨­å®š
# ==========================================================
with DAG(
    dag_id="d03_2_population",
    description="Population ETL Pipeline (with MySQL location mapping)",
    default_args=default_args,
    schedule="@monthly",  # æˆ– None, æˆ– cron è¡¨é”å¼
    start_date=datetime(2024, 12, 1),
    catchup=False,
    tags=["population", "ETL", "monthly"],
) as dag:

    # --------------------------
    # Extract
    # --------------------------
    def extract_task():
        print("ðŸ“Š [E] Extract - æŠ“å–å…§æ”¿éƒ¨äººå£çµ±è¨ˆè³‡æ–™ä¸­...")
        fetch_raw_data("/opt/airflow/data/raw/population")
        print("âœ… å·²æŠ“å–åŽŸå§‹äººå£è³‡æ–™")

    # --------------------------
    # Transform
    # --------------------------
    def transform_task():

        print("âš™ï¸ [T] Transform - æ¸…ç†ä¸¦å°æ‡‰ MySQL location...")
        # TODO è¦æ‰‹å‹•ä¿®æ”¹æª”å
        df_processed = transform_population_data(
            "/opt/airflow/data/raw/population/é„‰éŽ®æˆ¶æ•¸åŠäººå£æ•¸-114å¹´10æœˆ.xls"
        )
        print(f"âœ… å·²è½‰æ›äººå£è³‡æ–™ï¼Œå…± {len(df_processed)} ç­†")

    # --------------------------
    # Load
    # --------------------------
    def load_task():
        import pandas as pd

        df = pd.read_csv(
            "/opt/airflow/data/data/complete/store/type=population/store.csv"
        )

        print("ðŸ’¾ [L] Load - åŒ¯å…¥ MySQL ä¸­...")
        load(df)
        print("ðŸŽ‰ Population ETL Pipeline å®Œæˆï¼")

    # ==========================================================
    # Airflow Tasks
    # ==========================================================
    extract_population = PythonOperator(
        task_id="extract_population",
        python_callable=extract_task,
    )

    transform_population = PythonOperator(
        task_id="transform_population",
        python_callable=transform_task,
    )

    load_population = PythonOperator(
        task_id="load_population",
        python_callable=load_task,
    )

    # ä»»å‹™é †åº
    extract_population >> transform_population >> load_population
