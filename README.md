# TJR103 雲端資料工程師班 第一組專題：寵物資訊站

## 專案概要

本專題針對台灣六都寵物資源進行資料蒐集與分析，建立一個**寵物資訊匯集平台**，幫助飼主與業者了解各區域寵物資源分布及商業潛力。專案成果包含**自動化資料管線**、**GCS資料存放**、**BigQuery查詢**，並透過視覺化呈現分析結果。

## 研究動機

本專題嘗試解答幾個問題：

1. **飼主角度**：展示六都各地寵物資源數比較。
2. **業者角度**：分析六都各地區的商業潛力比較。
3. 建立一個**寵物資源匯集平台**，讓使用者能夠集中查詢寵物資源及資訊。

## 研究前提

1. **以六都為研究對象**：基於時間和資源考量，我們將目標放在六都，最小行政單位為「**區**」。六都是人口及資源最集中的區域，能最大化涵蓋可能的使用者。
2. **店家分類**：將店家分為「**寵物用品**」、「**寵物餐廳**」、「**寵物美容**」、「**寵物旅館**」、「**寵物醫院**」、「**寵物收容所**」等六大類，是目前最常見的寵物業店家。

## 技術架構與流程

1. **資料蒐集**：使用**python爬蟲套件**及**google maps API服務**蒐集各地店家資訊。
2. **ETL處理**：透過**pandas**清理、轉換，剔除無效資料，並轉換成可用的格式或型別。
3. **data pipeline自動化**：在GCP雲服務中建立VM，並部署容器化**Airflow**，透過DAG自動化管理、排程資料管線。完成的資料檔案上傳至**GCS**存檔。
4. **資料查詢與分析**：使用GCP上的**BigQuery服務**，直接連結GCS中的檔案建立外部資料表進行查詢。
5. **視覺化與成果展示**：使用**Tableau**製作視覺化報表及Dashboard，讓使用者輕鬆操作取得資料。

![專題架構圖](https://i.meee.com.tw/z7O31vG.png)

## 資料來源
| 資料類別       | 資料來源                                               |
| -------------- | ------------------------------------------------------ |
| 寵物登記數     | [農業部寵物登記管理資訊網-登記狀況](https://www.pet.gov.tw/Web/O302.aspx) |
| 公立收容所     | [動物保護資訊網](https://animal.moa.gov.tw/Frontend/PublicShelter) |
| 人口與行政區   | 內政部戶政司                                           |
| 寵物醫院       | [農業部動植物防疫檢疫署](https://ahis9.aphia.gov.tw/Veter/OD/HLIndex.aspx) |
|寵物旅館|[農業部寵物登記管理資訊網-合法寵物業者名單](https://www.pet.gov.tw/Web/BusinessList.aspx)|
| 其他店家       | Google Maps API                                        |


## 技術挑戰與解決方案

### 1. 店家資訊蒐集

網上尚未有寵物店家的集中資訊平台，故先透過「**政府資料**」及「**Google Maps API**」取得店家列表，再**使用Google Maps API服務取得所有店家詳細資料**。

- **優點**：資料格式統一、店家及使用者隨時可更新資料、貼近一般使用者。
- **缺點**：需大量使用Google Maps API服務。

### 2. 大量Google Maps API查詢
Google提供使用者每月固定的免費查詢次數，超過次數後便會另外計費。故使用次數將直接影響成本，控制使用次數便相當重要。對此我們有以下幾個做法來應對：

1. 考量店家資訊應該不會經常變動，故店家資訊更新頻率設定為**每月一次**。
2. 在搜尋店家資料前，先根據經緯度**排除不在六都範圍內的店家**，減少無效搜尋。
3. 每次更新時，**紀錄搜尋半徑、步長及搜尋次數**等，一方面得知每次更新時的花費；另一方面也能作為下次更新資料時的參考，適當調整搜尋設定來控制成本。

### 3. data pipeline自動化
由於資料更新需要大量的Google Maps API查詢，這個過程也需要一段不短的時間，另外也為了讓系統能做到24小時自動化，所以我們選擇使用雲端部署Airflow的方式進行。

1. **GCP**：GCP提供每位新的使用者3個月300美金的試用額度，不論是**時間**或**金額**都很適合這類小型專案的嘗試和開發。
2. **Compute Engine（VM）**：為了使軟體或工具能24小時運行，不受時間、地點限制，我們使用**VM**作為主體，並以部署**Docker Container**的方式，來取得較好的效能和穩定性。
3. **Apache Airflow**：雖然Linux中有Crontab排程工具，但功能陽春、難以準確追蹤每支程式執行的過程或結果。**Airflow**則提供完整的排成及追蹤功能，也有詳細的UI介面能夠即時確認程式執行的狀態。


## 指標計算與資料視覺化

## 未來展望
考量時間較短，以及開發經驗尚不成熟，本專案仍有許多未盡之事及改進空間。以下幾點是未來可改善的方向：

### 1. GCS/BigQuery使用
資料在進行ETL完成後，其實有將完成檔案上傳至GCS，並透過BigQuery建立External Table。但實際在資料的使用和分析上，仍是使用MySQL內的資料。如有機會應能更多使用這兩項GCP服務來達到更多便利性。
- **GCS**：穩定儲存檔案，應可涵蓋raw至complete各階段data的儲存。且功能類似hadoop/hive架構，同時兼具資料庫功能。
- **BigQuery**：可透過多種方式建立table，且若來源更新，table也會連動。不依賴VM或container也使穩定性更高。


### 2. SQL Operator使用
在資料分析階段選擇使用SQL指令在MySQL資料庫中進行計算及分析。這個部分並未放入Airflow中自動化執行，原因是對Airflow的功能尚未完全掌握，僅知道Airflow有內建SQL Operator可自動化執行SQL指令，但因時間緊湊而未深入了解，未來應能加以應用，使系統更完整的自動化。