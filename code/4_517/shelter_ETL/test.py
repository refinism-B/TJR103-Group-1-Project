import os
import requests
import pandas as pd
import urllib3
import platform
import subprocess
import re
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dotenv import load_dotenv
from sqlalchemy import create_engine

load_dotenv(dotenv_path=os.path.join(os.getcwd(), ".env"))
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# === API è¨­å®š ===
GOOGLE_API_KEY = "AIzaSyDLALYT1HVtKl3KD2s18IH8IB-trnDszmo"
GOOGLE_PLACES_SEARCH_URL = "https://maps.googleapis.com/maps/api/place/textsearch/json"
GOOGLE_PLACES_DETAILS_URL = "https://maps.googleapis.com/maps/api/place/details/json"
API_LINK = "https://data.moa.gov.tw/Service/OpenData/TransService.aspx?UnitId=2thVboChxuKs"

# === è¼¸å‡ºç›®éŒ„ ===
data_dir = os.path.join(os.getcwd(), "data")
os.makedirs(data_dir, exist_ok=True)
output_file = os.path.join(data_dir, "taiwan_pet_shelters_with_google.csv")


# === æŠ“å–è¾²æ¥­éƒ¨ API è³‡æ–™ ===
def get_api_json(url: str):
    res = requests.get(url, verify=False, timeout=15)
    res.raise_for_status()
    return res.json()


def clean_address(address):
    if pd.isna(address):
        return address
    address = re.sub(r"^\d{3,5}", "", address).strip()
    address = re.sub(r"[\(ï¼ˆ][^\)ï¼‰]*[\)ï¼‰]", "", address)
    return address.strip()


# === Google Maps æŸ¥è©¢ ===
def get_place_info(name, address):
    query = f"{name} {address}"
    params = {"query": query, "key": GOOGLE_API_KEY, "language": "zh-TW"}
    res = requests.get(GOOGLE_PLACES_SEARCH_URL, params=params, timeout=10)
    data = res.json()
    if data.get("results"):
        result = data["results"][0]
        return {
            "place_id": result.get("place_id"),
            "lat": result["geometry"]["location"]["lat"],
            "lng": result["geometry"]["location"]["lng"]
        }
    return None


def get_place_details(place_id):
    params = {
        "place_id": place_id,
        "fields": "rating,user_ratings_total,opening_hours,reviews,url,business_status",
        "language": "zh-TW",
        "key": GOOGLE_API_KEY,
    }
    res = requests.get(GOOGLE_PLACES_DETAILS_URL, params=params, timeout=10)
    data = res.json()
    result = data.get("result", {})

    opening_hours = None
    if result.get("opening_hours") and "weekday_text" in result["opening_hours"]:
        opening_hours = "; ".join(result["opening_hours"]["weekday_text"])

    business_status_map = {
        "OPERATIONAL": "ç‡Ÿæ¥­ä¸­",
        "CLOSED_TEMPORARILY": "æš«æ™‚é—œé–‰",
        "CLOSED_PERMANENTLY": "æ°¸ä¹…åœæ¥­"
    }
    business_status = business_status_map.get(result.get("business_status"), None)

    latest_review_time = None
    if result.get("reviews"):
        timestamps = [r.get("time") for r in result["reviews"] if r.get("time")]
        if timestamps:
            latest_review_time = datetime.fromtimestamp(max(timestamps)).strftime("%Y-%m-%d")

    return {
        "rating": result.get("rating"),
        "user_ratings_total": result.get("user_ratings_total"),
        "opening_hours": opening_hours,
        "business_status": business_status,
        "latest_review_time": latest_review_time,
        "gmap_url": result.get("url")
    }


# === æ¯é€±ç‡Ÿæ¥­æ™‚æ•¸è¨ˆç®— ===
def parse_opening_hours(opening_hours_str):
    if not opening_hours_str or pd.isna(opening_hours_str):
        return None
    total_hours = 0.0
    for day_info in opening_hours_str.split("; "):
        try:
            parts = day_info.split(": ")
            if len(parts) != 2:
                continue
            time_part = parts[1]
            if any(kw in time_part for kw in ["ä¼‘æ¯", "æœªç‡Ÿæ¥­", "å…¬ä¼‘", "ä¸ç‡Ÿæ¥­"]):
                continue
            time_ranges = [r.strip() for r in time_part.split(",") if "â€“" in r]
            for time_range in time_ranges:
                start_str, end_str = [t.strip() for t in time_range.split("â€“")]
                start = datetime.strptime(start_str, "%H:%M")
                end = datetime.strptime(end_str, "%H:%M")
                if end < start:
                    end = end.replace(day=start.day + 1)
                total_hours += (end - start).seconds / 3600
        except Exception:
            continue
    return round(total_hours, 2)


# === Google å¤šåŸ·è¡Œç·’æŸ¥è©¢ ===
def enrich_with_google_info(row):
    name, addr = row["æ”¶å®¹æ‰€åç¨±"], row["åœ°å€"]
    try:
        place_info = get_place_info(name, addr)
        if not place_info:
            print(f"âš ï¸ æ‰¾ä¸åˆ° {name} çš„ Google è³‡æ–™")
            return {
                "Google è©•åˆ†": None, "è©•åˆ†äººæ•¸": None, "ç‡Ÿæ¥­æ™‚é–“": None,
                "Place ID": None, "ç¶“åº¦": None, "ç·¯åº¦": None,
                "ç‡Ÿæ¥­ç‹€æ…‹": None, "æœ€æ–°è©•è«–æ—¥æœŸ": None, "GMap ç¶²å€": None
            }

        details = get_place_details(place_info["place_id"])
        print(f"âœ… {name} â†’ {details['rating']}â­ ({details['user_ratings_total']} å‰‡)")
        return {
            "Google è©•åˆ†": details["rating"],
            "è©•åˆ†äººæ•¸": details["user_ratings_total"],
            "ç‡Ÿæ¥­æ™‚é–“": details["opening_hours"],
            "Place ID": place_info["place_id"],
            "ç¶“åº¦": place_info["lng"],
            "ç·¯åº¦": place_info["lat"],
            "ç‡Ÿæ¥­ç‹€æ…‹": details["business_status"],
            "æœ€æ–°è©•è«–æ—¥æœŸ": details["latest_review_time"],
            "GMap ç¶²å€": details["gmap_url"]
        }
    except Exception as e:
        print(f"âš ï¸ æŸ¥è©¢å¤±æ•—ï¼š{name} ({addr}) - {e}")
        return {
            "Google è©•åˆ†": None, "è©•åˆ†äººæ•¸": None, "ç‡Ÿæ¥­æ™‚é–“": None,
            "Place ID": None, "ç¶“åº¦": None, "ç·¯åº¦": None,
            "ç‡Ÿæ¥­ç‹€æ…‹": None, "æœ€æ–°è©•è«–æ—¥æœŸ": None, "GMap ç¶²å€": None
        }


# === ä¸»æµç¨‹ ===
def main():
    print("ğŸ¾ æ­£åœ¨å¾è¾²æ¥­éƒ¨ API æŠ“å–è³‡æ–™...")
    data = get_api_json(API_LINK)
    df = pd.DataFrame(data)
    print(f"ğŸ“‹ å…±å–å¾— {len(df)} ç­†å…¨å°æ”¶å®¹æ‰€è³‡æ–™")

    df = df[["ShelterName", "CityName", "Address", "Phone"]].copy()
    df.rename(columns={
        "ShelterName": "æ”¶å®¹æ‰€åç¨±",
        "CityName": "ç¸£å¸‚",
        "Address": "åœ°å€",
        "Phone": "é›»è©±",
    }, inplace=True)

    print("ğŸ” æŸ¥è©¢ Google Maps è³‡æ–™ä¸­ï¼ˆå¤šåŸ·è¡Œç·’ï¼‰...")
    with ThreadPoolExecutor(max_workers=6) as executor:
        futures = {executor.submit(enrich_with_google_info, row): idx for idx, row in df.iterrows()}
        results = {}
        for future in as_completed(futures):
            idx = futures[future]
            results[idx] = future.result()

    df = pd.concat([df, pd.DataFrame.from_dict(results, orient="index")], axis=1)

    df["åœ°å€"] = df["åœ°å€"].apply(clean_address)
    df["æ¯é€±ç‡Ÿæ¥­æ™‚æ•¸"] = df["ç‡Ÿæ¥­æ™‚é–“"].apply(parse_opening_hours)

    print("ğŸ§© æ¬„ä½çµ±ä¸€èˆ‡æ’åºä¸­...")

    df.rename(columns={
        "æ”¶å®¹æ‰€åç¨±": "name",
        "ç¸£å¸‚": "loc_id",
        "åœ°å€": "address",
        "é›»è©±": "phone",
        "ç‡Ÿæ¥­ç‹€æ…‹": "buss_status",
        "æ¯é€±ç‡Ÿæ¥­æ™‚æ•¸": "op_hours",
        "Google è©•åˆ†": "rating",
        "è©•åˆ†äººæ•¸": "rating_total",
        "æœ€æ–°è©•è«–æ—¥æœŸ": "newest_review",
        "ç¶“åº¦": "longitude",
        "ç·¯åº¦": "latitude",
        "GMap ç¶²å€": "map_url",
        "Place ID": "place_id"
    }, inplace=True)

    df["loc_id"] = [f"SH{str(i+1).zfill(4)}" for i in range(len(df))]
    df["category_id"] = 1
    df["update_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    df = df[[
        "name", "buss_status", "loc_id", "address", "phone",
        "op_hours", "category_id", "rating", "rating_total",
        "newest_review", "longitude", "latitude", "map_url",
        "place_id", "update_time"
    ]]

    df.to_csv(output_file, index=False, encoding="utf-8-sig")
    print(f"ğŸ“Š å·²æˆåŠŸè¼¸å‡ºï¼š{output_file}")
    print(f"âœ… å…± {len(df)} ç­†è³‡æ–™å®Œæˆ")
    return df


if __name__ == "__main__":
    df = main()

    # === åŒ¯å…¥ MySQL ===
    username = os.getenv("MYSQL_USERNAME")
    password = os.getenv("MYSQL_PASSWORD")
    target_ip = os.getenv("MYSQL_IP")
    target_port = int(os.getenv("MYSQL_PORTT"))
    db_name = os.getenv("MYSQL_DB_NAME")

    engine = create_engine(f"mysql+pymysql://{username}:{password}@{target_ip}:{target_port}/{db_name}")
    try:
        df.to_sql(name="pet_shelter", con=engine, if_exists="replace", index=False)
        print("âœ… å·²æˆåŠŸåŒ¯å…¥ MySQL è³‡æ–™è¡¨ï¼špet_shelter")
    except Exception as e:
        print(f"âŒ åŒ¯å…¥ MySQL å¤±æ•—ï¼š{e}")
